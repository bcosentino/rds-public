# Why Statistics? The Problem of Uncertainty

## The nature of science

How do we know what we know? Ultimately this book is about scientific research as one method of advancing knowledge. When we do research, we're interested in seeking the truth. Scientists aren't alone in this truth-seeking venture. Journalists, philosophers, mathematicians, artists, historians, and so on; they have the same goal, just different questions and methods of inquiry. There's even a branch of philosophy called **epistemology**, which aims to seek truth about the methods of seeking truth.

So what distinguishes science from these other fields of truth-seeking? Defining science remains a contentious topic among philosophers, but at its most basic level, science is a process of testing ideas about the natural world with **empirical data**. Like other methods of inquiry, scientists use logic and reasoning to develop research questions, ideas, and expectations, but what separates science from non-science is the confrontation of those reasoned ideas with observations from the natural world. 

In one sense, we're all scientists, as Andrew Jaffe aptly points out in his book **The Random Universe**. Daily life requires confronting ideas with data. Is it going to rain today? If you see a lot of cloud cover outside, your suspicion of rain might be increased. Or imagine you had a hard time falling asleep at night. You remember that you had an espresso after dinner, and that makes you think drinking coffee at night might affect your ability to fall asleep. Single observations like this are **anecdotes**, and won't be of much use in isolation. But imagine that you notice you have a hard time falling asleep nearly every time you have an espresso after dinner, no matter what you ate for dinner or what else you might have done during that day. Those repeated observations, and the patterns that emerge from those observations - can be extremely useful for testing ideas. This book is an introduction to the methods of a) designing studies to collect data in a systematic manner (**research design**), and b) analyzing and interpreting those data to provide insight into a research question (**statistics**). 

Before we get into the weeds on research design and statistics, I want to make two points about "how we know what we know". First, notice that I defined science as a *process*, not a body of knowledge. Uncovering truths about the universe with science is an aspirational goal. The problem is that there's no certainty about the ideas we test in science. A scientific study can advance our current state of knowledge, but it can't find truth with certainty. We have to be open to the idea that knowledge about the world based on science might be wrong. Science is a process of investigating ideas with data in the presence of **uncertainty**. All knowledge is contingent. What we know at one point in time can be revised or overturned by what we learn from later investigations. If you're looking for a cookbook to find "the definitive answer", this isn't it.

Second, doing science doesn't guarantee the advancement of knowledge. Research isn't a production line where you provide all the inputs and get knowledge at the end. Knowledge is produced through structured inquiry *and* debate with others. Indeed, science is a process that involves logic, reasoning, and structured examination of observations, but it also involves social interactions, including all the cognitive biases of being a human (Hull 1988). The good thing is that knowledge isn't decided by any one person. The impact of your research on knowledge depends on convincing your peers. A scientist may well reach a conclusion in a biased manner, but the great thing about science is that there's a community of other scientists out there, and not all of them share the same biases. If the community is unswayed by the idea, it will be left behind. Knowledge is decided by the collective, not the individual. Jonathan Rauch calls this system of knowledge production **liberal science**, because while it encompasses the sciences, it also includes other methods of inquiry, such as history and journalism. The bottom line is that knowledge is advanced through a *public process* of evaluation. No one study, and no one individual has the final say or authority over the state of knowledge. When you do research, the onus is on you to do high-quality research that will stand up to public criticism and convince your peers. 

I say this not to scare anyone off from doing science! Science is fun and rewarding. But it's also hard. Composing a clear research question, designing a systematic approach to collect data, and analyzing and interpreting data are not straightforward tasks! There are compelling arguments that the "publish or perish" pressure in academia has incentivized bad research practices that ultimately contribute to the publication of studies that are wrong [^ch01-1]. Scientists have paid too little attention to research practices and workflows that prioritize accuracy of scientific inference over throughput. I can look back at some of my own publications and find examples where I now questions the decisions I made, and where I'd almost certainly approach the problem differently today. I wish more attention was paid to these issues when I was learning about research design and statistics, and that has been a motivating force to write this book. My hope is that this book will help empower you with basic skills needed to design and conduct high-quality science and to effectively evaluate the science of others. 

[^ch01-1]: See [Horton 2015](https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(15)60696-1/fulltext) and [McElreath 2023](https://elevanth.org/2023_06_13_Science_Dumpster_Fire.html) for starters.

## Goals of scientific research

In 1986 a study by [Menkes et al.](https://www.nejm.org/doi/full/10.1056/NEJM198611133152003) was published in the prestigious New England Journal of Medicine on the relationship between lung cancer risk and dietary factors, specifically intake of vitamin A, vitamin E, beta-carotene, and selenium. One finding was that the risk of lung cancer was negatively associated with beta carotene levels in the blood. In other words, lung cancer was observed far less often in people with the highest levels of beta carotene. When summarizing the conclusions of their study, the authors wrote that their study suggests "low serum levels of beta carotene increase the risk of subsequent squamous-cell carcinoma of the lung". The study has been [cited 762 times](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C33&q=Serum+beta-carotene%2C+vitamins+A+and+E%2C+selenium%2C+and+the+risk+of+lung+cancer+&btnG=) as of this writing.

Ten years later, the New England Journal of Medicine published another study on the relationship between lung cancer risk and beta carotene by [Omenn et al.](https://www.nejm.org/doi/full/10.1056/NEJM199605023341802), this time showing a **positive** relationship. The relationship was weak, but the direction of the relationship was opposite of that reported by the 1986 study. What gives?

Are either of these studies wrong? Well, that really depends on the goal of each study. If the goal was to test whether beta-carotene has a **causal effect** on lung cancer risk, then surely one of the studies must be wrong. For reasons we'll get into later, the 1996 study had a more appropriate research design for inferring a causal effect of beta carotene, despite the claim I quoted from the authors of the 1986 study.

But what if a cause-and-effect relationship wasn't of interest? Instead, what if the researcher's primary goal was to design a statistical model that could accurately **predict** who is most likely to have lung cancer? The 1986 study found that individuals with the highest levels of beta-carotene had four times the risk of lung cancer as individuals with the lowest levels of beta-carotene. Beta-carotene may not be an important cause of lung cancer, but it might be a useful marker for lung cancer risk if it's associated with other variables that cause lung cancer. For example, perhaps high beta-carotene is found among individuals who exercise a lot and don't smoke, and it's those latter factors that causally explain lung cancer risk.

The takeaway here is that the way we assess the value of a study depends heavily on its research goal. Are we seeking to explain, predict, or describe? Textbooks and courses on statistics tend to start with an overview of different types of data and proceed to recommend specific statistical tests based on the type of data. They often bypass discussion about the nature of the research question and implications for how the study should be designed. Frankly I've used that approach myself in my own statistics course. But here's the problem: it suggests that *how* you should analyze the data has little relationship to *why* you're analyzing the data. The truth is that the way you collect and analyze data often depends on your research goals.

## Three general goals of scientific research

At the broadest level of investigation, there are three main types of scientific research: explanation, prediction, and description ([Hernan et al. 2019](https://www.tandfonline.com/doi/full/10.1080/09332480.2019.1579578), [Hamaker et al. 2020](https://www.sciencedirect.com/science/article/pii/S1878929320301171). Sometimes we simply want to describe some aspect of the world, other times we want to forecast something in the future, and still other times we want to explain the causes behind certain patterns or outcomes. Let's break down these three goals:

1.  **Description:** The simplest of the three, the goal of description is to numerically summarize a quantity of interest. For example, what is the proportion of women in four-year colleges in the United States? What is the typical weight of a pint of Ben and Jerry's ice cream? What is the typical body temperature of healthy adults? In each of these cases, the goal is simply to describe some aspect of the world numerically.

2.  **Prediction:** The goal of predictive research is to forecast some future event based on historical or current data. Here the main objective is to accurately project what an outcome will be in the future. For example, weather forecasting, predicting stock market movements, or estimating the growth of a newborn baby are all examples where prediction is the main goal.

    In prediction, the relationship between variables doesnâ€™t need to be causal; what matters is how well the variables help forecast the outcome. For example, beta-carotene levels might predict lung cancer risk even if they beta-carotene levels are not a cause of lung cancer. In this sense, studies like the one from 1986 can still be valuable. Problems arise however when the goal of a study - like the 1986 study - is explanation but the research design is more targeted towards prediction.

3.  **Explanation (i.e., causal inference):** Here the goal is to understand whether a particular variable is a cause of some other variable. Does the dietary intake of beta carotene affect lung cancer risk? Does being in a wealthy school district affect the likelihood of being admitted to an Ivy League university? Does restoring forest increase biodiversity? These are all questions about the causal relationship between variables.

    There is much philosophical discussion about the meaning of **cause**. In this book, I will use "cause" to refer to the impact of an intervention of one variable on some other variable. Examining causation requires one to consider a **counterfactual** world. What would happen to an outcome if the world was different? For example, would the likelihood of being admitted to an Ivy League university differ if individuals in a wealthly school district were actually in a poor school district? For those individuals in a wealthy school district, the counterfactual is being in a poor school district. For individuals in a poor school district, the counterfactual is being in a wealthy school district. In this case, causal inference requires examining the impact of changing the wealth level of a school district on Ivy league admittance.

    Causal inference was the primary focus of the 1996 study on beta carotene and lung cancer risk. They conducted an experiment in which they some individuals were given beta-carotene, and others were not. For reasons we will discuss later, the essential element of the study design was the random assignment of the beta-carotene treatment to individuals, which is the defining attribute of what we will call an **experiment**. Then they looked at the outcome, whether or not individuals developed lung cancer. This research design made it easier to make causal conclusions about beta carotene compared to the 1986 study, which was not an experiment. As we will find out, causal inference is more challenging when scientists can't do an experiment, but it's not impossible!

    I think it's fair to say that most scientists are interested in causation at some level. As we'll find out, making causal conclusions can be tricky, especially in disciplines where experimentation is hard or impossible. Sometimes scientists are hesitant to use the word "cause" in their papers, even if explanation is their goal. Instead they'll use other phrases that clearly imply causation without being direct about it; "X affects Y", or "X drives Y", or "X determines Y", or "X contributes to Y", or "X plays a role in Y", etc. Suffice to say that the softened language doesn't get around the challenges of making causal inferences.

## Research design and statistical analysis depend on your goal

The three main research goals - description, prediction, explanation - are not mutually exclusive. Many research projects involve more than one of these goals, but each has different implications for how you should collect and analyze data. Description is probably the most straightforward in that it often involves quantifying simple summary statistics and reporting patterns. For example, if you want to describe a typical weight of a pint of Ben and Jerry's ice cream, you need to measure the weights of some pints.

Whereas descriptive research often stops with simple summary statistics, prediction goes a step further in using the data to forecast the unseen. For example, a model might be built using beta carotene levels and other aspects of lifestyle, occupation, and environmental exposure to predict the likelihood of developing lung cancer. As such, prediction tends to be a more complicated goal than descriptive research, requiring statistical procedures to predict the future, and validation to ensure the predictions are accurate.

Causal inference tends to be a bit more complicated. Consider the 1986 study where the goal is to understand whether beta carotene levels affect lung cancer risk, but without an experimental intervention. Should you just look at the raw association between lung cancer occurrence and beta carotene among all the individuals in the study? Should you separate out the smokers from the nonsmokers and look at the association between lung cancer and beta carotene separately in each group? What about other differences between the individuals in the study? Occupation (which vary in exposure to contaminants), radon gas levels in the home, air pollution levels in the neighborhood, lifestyle factors like exercise, etc.. The list of other potential causes of lung cancer goes on and on, and some of these causes may be causally related themselves!

## You can't escape uncertainty in science

No matter what your research goal is, one thing I'd like to convince you of in this book is that using data to draw conclusions about your question is fraught with uncertainty. Too often statistical analysis - the methods we use to leverage data as evidence for our research questions - is presented like a recipe: 1) State a question and hypothesis, 2) Collect data, 3) Run a test, 4) Use a statistical criterion to decide if the result is "significant" (usually a P-value), and 5) Conclude whether the data confirms or rejects the hypothesis. If only it were so simple. 

Consider a study evaluating if a vaccine is effective for reducing the prevalence of an infection. The recipe version of statistics starts by assuming the vaccine has no effect (the "null hypothesis"), then asks if the observed data would be unusual if the null hypothesis was true. One problem with this approach is that it gives the impression that statistics is a form of Euclidean proof. In geometry, Euclid started with axioms and deduced conclusions that must be true. In the standard approach to statistic, we begin with assumptions - a null hypothesis and statistical model - and then deduce the kind of observations that would be expected if the assumptions were true. The commonly used P-value is a measure of how surprising the data would be *if* the null hypothesis was true, but too often it's misunderstood as the probability of the null hypothesis being true. This misunderstanding encourages scientists to make binary conclusions - either the null hypothesis is rejected or not - as if statistical analysis provides a definitive verdict on ideas. 

The approach to statistics in this book attempts to reverse the emphasis. In philosophy, the use of specific observations to draw general conclusions is called induction. Ultimately that's what we're doing in science. But we will emphasize here that the conclusions we draw based on our observations are never certain. We cannot definitively reject or prove an idea. Why not? The challenge is that our conclusions depend not only on the data we observe, but also on the *assumptions* we make when we connect ideas to data. How did we sample a population? How did we measure the data? What sources of variation did we control or fail to control as part of our study design? The tools we use to connect data to ideas are never perfect, and so honest conclusions about ideas must be made probabilistically. In this book, we will attempt to answer questions not in a binary way, but rather as a matter of degree. How strongly do the data we observe sway our view on the plausibility of an idea? To do this, we will need a language to describe our uncertainty about ideas based on data, and that language will be probability. 
This is why statistics exists as a discipline. Its goal is not to manufacture definitive answers to research questions, but to provide a rigorous framework for reasoning about ideas in the face of uncertainty. 


